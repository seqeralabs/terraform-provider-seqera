## Datasets
resource "seqera_datasets" "my_datasets" {
  description  = "Terraform created dataset"
  name         = "terraform-dataset"
  workspace_id = var.workspace_id
}


## Pipeline Secrets
resource "seqera_pipeline_secret" "my_pipelinesecret" {
  name         = "test_terraform_secret"
  value        = "SECRET_VALUE"
  workspace_id = var.workspace_id
}

## Credentials
resource "seqera_credential" "aws_credential" {

  name          = "test_credential"
  provider_type = "aws"
  workspace_id  = var.workspace_id
  description   = "AWS credentials generated by terraform"
  keys = {
    aws = {
      access_key      = var.access_key
      secret_key      = var.secret_key
      assume_role_arn = var.iam_role
    }
  }
}

## Compute Environments
resource "seqera_compute_env" "aws_batch_compute_env" {
  workspace_id = var.workspace_id

  compute_env = {
    name           = "aws-batch-compute-env"
    description    = "AWS Batch compute environment for bioinformatics workflows"
    platform       = "aws-batch"
    credentials_id = resource.seqera_credential.aws_credential.credentials_id

    config = {
      aws_batch = {
        discriminator = "aws-batch"
        region        = "us-east-1"
        work_dir      = var.work_dir


        # Head job configuration
        head_job_cpus      = 2
        head_job_memory_mb = 4096

        # Features
        fusion2_enabled = true
        wave_enabled    = true


        # Optional: Forge configuration for auto-scaling
        forge = {
          dispose_on_deletion = true
          type                = "EC2" # or "SPOT" for cost savings
          alloc_strategy      = "BEST_FIT_PROGRESSIVE"

          # Instance configuration
          instance_types = ["m5.large", "m5.xlarge", "m5.2xlarge"]
          min_cpus       = 0
          max_cpus       = 1000


          ebs_auto_scale = false
          gpu_enabled    = false
          arm64_enabled  = false

          #   # Optional: EC2 key pair for debugging
          #   ec2_key_pair = var.ec2_key_pair_name
        }

        # Optional: Custom Nextflow configuration
        # nextflow_config = <<-EOF
        #   process {
        #     executor = 'awsbatch'
        #     queue = 'default'
        #   }
        #   aws {
        #     region = 'us-east-1''
        #     batch {
        #       cliPath = '/home/ec2-user/miniconda/bin/aws'
        #     }
        #   }
        # EOF

        # Optional: Pre and post-run scripts
        pre_run_script = <<-EOF
          #!/bin/bash
          echo "Starting workflow execution..."
          # Add any setup commands here
        EOF

        post_run_script = <<-EOF
          #!/bin/bash
          echo "Workflow execution completed!"
          # Add any cleanup commands here
        EOF
      }
    }
  }
}

resource "seqera_primary_compute_env" "my_primarycomputeenv" {
  compute_env_id = resource.seqera_compute_env.aws_batch_compute_env.compute_env_id
  workspace_id   = var.workspace_id
}

## Data Link
resource "seqera_data_link" "my_datalink" {
  credentials_id    = resource.seqera_credential.aws_credential.credentials_id
  description       = "data link created by Terraform"
  name              = "terraform-datalink"
  provider_type     = "aws"
  public_accessible = false
  type              = "bucket"
  workspace_id      = var.workspace_id
  resource_ref      = var.work_dir
}

## Actions
resource "seqera_action" "my_action" {
  launch = {
    compute_env_id  = resource.seqera_compute_env.aws_batch_compute_env.compute_env_id
    config_profiles = []
    config_text     = ""
    pipeline        = "https://github.com/nf-core/sarek"
    pre_run_script  = <<-EOF
      #!/bin/bash
      echo "Starting workflow execution..."
      # Add any setup commands here
    EOF

    post_run_script = <<-EOF
      #!/bin/bash
      echo "Workflow execution completed!"
      # Add any cleanup commands here
    EOF

    pull_latest = true
    resume      = true
    revision    = "master"
    #run_name             = "...my_run_name..." this should be auto generated
    work_dir = var.work_dir
  }
  name         = "terraform-hello-world-action"
  workspace_id = var.workspace_id
  source       = "tower"
}

## Pipelines

resource "seqera_pipeline" "hello_world_minimal" {
  description = "Hello world pipeline generated by terraform"
  name        = "terraform-hello-world"
  launch = {
    compute_env_id  = resource.seqera_compute_env.aws_batch_compute_env.compute_env_id
    config_profiles = []
    head_job_cpus   = 6
    # head_job_memory_mb = 32
    pipeline    = "https://github.com/nextflow-io/hello"
    pull_latest = true
    resume      = false
    revision    = "master"
    work_dir    = var.work_dir
  }
  workspace_id = var.workspace_id
}



## Launch a workflow
resource "seqera_workflows" "my_workflows" {
  compute_env_id = resource.seqera_compute_env.aws_batch_compute_env.compute_env_id
  pipeline       = "nextflow-io/hello"
  work_dir       = var.work_dir
  workspace_id   = var.workspace_id
  depends_on     = [seqera_compute_env.aws_batch_compute_env]

}


## Data Studi

resource "seqera_studios" "my_datastudios" {
  compute_env_id = resource.seqera_compute_env.aws_batch_compute_env.compute_env_id
  description    = "Data studio"
  name           = "Terraform-Data-Studio"
  configuration = {
    #conda_environment = ""
    # cpu               = 6
    # gpu               = 8
    # lifespan_hours    = 5
    # memory            = 9
    # mount_data = [
    #   "..."
    # ]
  }
  workspace_id         = var.workspace_id
  data_studio_tool_url = "public.cr.seqera.io/platform/data-studio-jupyter:4.2.5-0.8"
  depends_on           = [seqera_compute_env.aws_batch_compute_env]
}


resource "seqera_labels" "my_labels" {
  is_default   = false
  name         = "terraform-test-label"
  resource     = true
  value        = "terraform-label-value"
  workspace_id = var.workspace_id
}
