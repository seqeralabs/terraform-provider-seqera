---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "seqera_aws_batch_compute_env Resource - terraform-provider-seqera"
subcategory: ""
description: |-
  Manage AWS Batch compute environments in Seqera Platform.
  AWS Batch compute environments provide scalable compute capacity for running
  Nextflow workflows on AWS using the AWS Batch service.
---

# seqera_aws_batch_compute_env (Resource)

Manage AWS Batch compute environments in Seqera Platform.

AWS Batch compute environments provide scalable compute capacity for running
Nextflow workflows on AWS using the AWS Batch service.

## Example Usage

```terraform
resource "seqera_aws_batch_compute_env" "my_awsbatchcomputeenv" {
  config = {
    cli_path         = "...my_cli_path..."
    compute_job_role = "...my_compute_job_role..."
    compute_queue    = "...my_compute_queue..."
    enable_fusion    = false
    enable_wave      = false
    execution_role   = "...my_execution_role..."
    forge = {
      allocation_strategy  = "BEST_FIT"
      bid_percentage       = 28
      dispose_on_deletion  = false
      ebs_auto_scale       = false
      ebs_block_size       = 14017
      ec2_key_pair         = "...my_ec2_key_pair..."
      efs_create           = false
      efs_id               = "...my_efs_id..."
      efs_mount            = "...my_efs_mount..."
      fargate_head_enabled = false
      forge_type           = "EC2"
      fsx_mount            = "...my_fsx_mount..."
      fsx_name             = "...my_fsx_name..."
      fsx_size             = 6
      gpu_enabled          = false
      instance_types = [
        "..."
      ]
      max_cpus = 10
      min_cpus = 0
      security_groups = [
        "..."
      ]
      subnets = [
        "..."
      ]
      vpc_id = "...my_vpc_id..."
    }
    head_job_cpus      = 2
    head_job_memory_mb = 9
    head_job_role      = "...my_head_job_role..."
    head_queue         = "...my_head_queue..."
    post_run_script    = "...my_post_run_script..."
    pre_run_script     = "...my_pre_run_script..."
  }
  credentials_id = "...my_credentials_id..."
  description    = "...my_description..."
  label_ids = [
    3
  ]
  name           = "...my_name..."
  region         = "...my_region..."
  work_directory = "...my_work_directory..."
  workspace_id   = 10
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `credentials_id` (String) AWS credentials ID to use for accessing AWS services. Requires replacement if changed.
- `name` (String) Display name for the compute environment (max 100 characters). Requires replacement if changed.
- `region` (String) AWS region where the Batch compute environment will be created.
Examples: us-east-1, eu-west-1, ap-southeast-2
Requires replacement if changed.
- `work_directory` (String) S3 bucket path for Nextflow work directory where intermediate files will be stored.
Format: s3://bucket-name/path
Example: s3://my-nextflow-bucket/work
- `workspace_id` (Number) Workspace numeric identifier

### Optional

- `config` (Attributes) (see [below for nested schema](#nestedatt--config))
- `description` (String) Optional description of the compute environment
- `label_ids` (List of Number) Requires replacement if changed.

### Read-Only

- `compute_env_id` (String) Unique identifier for the compute environment
- `message` (String) Status message or error details
- `status` (String) Current status of the compute environment

<a id="nestedatt--config"></a>
### Nested Schema for `config`

Optional:

- `cli_path` (String) Path to AWS CLI on compute instances. Default: "/home/ec2-user/miniconda/bin/aws"
- `compute_job_role` (String) IAM role ARN for compute jobs. Jobs assume this role during execution.
Format: arn:aws:iam::account-id:role/role-name
- `compute_queue` (String) Name of the AWS Batch compute queue
- `enable_fusion` (Boolean) Enable Fusion v2 for virtual file system. Fusion provides virtual file system
for efficient S3 access and improves performance by lazy loading files.
Default: false
- `enable_wave` (Boolean) Enable Wave containers service. Wave builds and manages container images on-demand.
When enable_wave is true, enable_fusion must be explicitly set.
Default: false
- `execution_role` (String) IAM role ARN for Batch execution (pulling container images, writing logs).
Format: arn:aws:iam::account-id:role/role-name
- `forge` (Attributes) AWS Forge configuration for compute resources (see [below for nested schema](#nestedatt--config--forge))
- `head_job_cpus` (Number) Number of CPUs allocated for the head job. Default: 1
- `head_job_memory_mb` (Number) Memory allocation for the head job in MB. Default: 1024
- `head_job_role` (String) IAM role ARN for the head job
- `head_queue` (String) Name of the head job queue
- `post_run_script` (String) Bash script to run after workflow execution completes.
Use for cleanup, archiving results, etc.
- `pre_run_script` (String) Bash script to run before workflow execution begins.
Use for environment setup, loading modules, etc.

<a id="nestedatt--config--forge"></a>
### Nested Schema for `config.forge`

Optional:

- `allocation_strategy` (String) Strategy for allocating compute resources.
SPOT_CAPACITY_OPTIMIZED only valid when forge_type is SPOT.
must be one of ["BEST_FIT", "BEST_FIT_PROGRESSIVE", "SPOT_CAPACITY_OPTIMIZED", "SPOT_PRICE_CAPACITY_OPTIMIZED"]
- `bid_percentage` (Number) Maximum percentage of On-Demand price to pay for Spot instances (0-100).
Only applicable when forge_type is SPOT.
- `dispose_on_deletion` (Boolean) Dispose of AWS Batch resources when compute environment is deleted. Default: true
- `ebs_auto_scale` (Boolean) Enable automatic EBS volume expansion. Default: false
- `ebs_block_size` (Number) Size of EBS root volume in GB (minimum 8 GB, maximum 16 TB). Default: 50
- `ec2_key_pair` (String) EC2 key pair name for SSH access to compute instances
- `efs_create` (Boolean) Automatically create an EFS file system. Default: false
- `efs_id` (String) EFS file system ID to mount.
Format: fs- followed by hexadecimal characters
- `efs_mount` (String) Path where EFS will be mounted in the container. Default: "/mnt/efs"
- `fargate_head_enabled` (Boolean) Use Fargate for head job instead of EC2.
Reduces costs by running head job on serverless compute.
Default: false
- `forge_type` (String) Type of compute instances to provision:
- SPOT: Use EC2 Spot instances (cost-effective, can be interrupted)
- EC2: Use On-Demand EC2 instances (reliable, higher cost)
- FARGATE: Use AWS Fargate serverless compute
Default: "EC2"; must be one of ["SPOT", "EC2", "FARGATE"]
- `fsx_mount` (String) Path where FSx will be mounted in the container. Default: "/fsx"
- `fsx_name` (String) FSx for Lustre file system name
- `fsx_size` (Number) Size of FSx file system in GB
- `gpu_enabled` (Boolean) Enable GPU support for compute instances.
When enabled, GPU-capable instance types will be selected.
Default: false
- `instance_types` (List of String) List of EC2 instance types to use.
Examples: ["m5.xlarge", "m5.2xlarge"], ["c5.2xlarge"], ["p3.2xlarge"]
Default: ["optimal"] - AWS Batch selects appropriate instances
- `max_cpus` (Number) Maximum number of CPUs available in the compute environment.
Subject to AWS service quotas.
Default: 256
- `min_cpus` (Number) Minimum number of CPUs to maintain in the compute environment.
Setting to 0 allows environment to scale to zero when idle.
Default: 0
- `security_groups` (List of String) List of security group IDs to attach to compute instances
- `subnets` (List of String) List of subnet IDs for compute instances.
Subnets must be in the specified VPC. Use multiple subnets for high availability.
Requires replacement if changed.
- `vpc_id` (String) VPC ID where compute environment will be deployed.
Format: vpc- followed by hexadecimal characters
Requires replacement if changed.

## Import

Import is supported using the following syntax:

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = seqera_aws_batch_compute_env.my_seqera_aws_batch_compute_env
  id = jsonencode({
    compute_env_id = "..."
    workspace_id = 0
  })
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
terraform import seqera_aws_batch_compute_env.my_seqera_aws_batch_compute_env '{"compute_env_id": "...", "workspace_id": 0}'
```
