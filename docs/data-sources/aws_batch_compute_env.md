---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "seqera_aws_batch_compute_env Data Source - terraform-provider-seqera"
subcategory: ""
description: |-
  Manage AWS Batch compute environments in Seqera Platform.
  AWS Batch compute environments provide scalable compute capacity for running
  Nextflow workflows on AWS using the AWS Batch service.
---

# seqera_aws_batch_compute_env (Data Source)

Manage AWS Batch compute environments in Seqera Platform.

AWS Batch compute environments provide scalable compute capacity for running
Nextflow workflows on AWS using the AWS Batch service.

## Example Usage

```terraform
data "seqera_aws_batch_compute_env" "my_awsbatchcomputeenv" {
  attributes = [
    "labels"
  ]
  workspace_id = 10
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `workspace_id` (Number) Workspace numeric identifier

### Optional

- `attributes` (List of String) Additional attribute values to include in the response (`labels`). Returns an empty value (`labels: null`) if omitted.

### Read-Only

- `compute_env_id` (String) Compute environment string identifier
- `config` (Attributes) (see [below for nested schema](#nestedatt--config))
- `credentials_id` (String) AWS credentials ID to use for accessing AWS services
- `description` (String) Optional description of the compute environment
- `message` (String) Status message or error details
- `name` (String) Display name for the compute environment (max 100 characters)
- `region` (String) AWS region where the Batch compute environment will be created.
Examples: us-east-1, eu-west-1, ap-southeast-2
- `status` (String) Current status of the compute environment
- `work_directory` (String) S3 bucket path for Nextflow work directory where intermediate files will be stored.
Format: s3://bucket-name/path
Example: s3://my-nextflow-bucket/work

<a id="nestedatt--config"></a>
### Nested Schema for `config`

Read-Only:

- `cli_path` (String) Path to AWS CLI on compute instances
- `compute_job_role` (String) IAM role ARN for compute jobs. Jobs assume this role during execution.
Format: arn:aws:iam::account-id:role/role-name
- `compute_queue` (String) Name of the AWS Batch compute queue
- `enable_fusion` (Boolean) Enable Fusion v2 for virtual file system. Fusion provides virtual file system
for efficient S3 access and improves performance by lazy loading files.
- `enable_wave` (Boolean) Enable Wave containers service. Wave builds and manages container images on-demand.
When enable_wave is true, enable_fusion must be explicitly set.
- `execution_role` (String) IAM role ARN for Batch execution (pulling container images, writing logs).
Format: arn:aws:iam::account-id:role/role-name
- `forge` (Attributes) AWS Forge configuration for compute resources (see [below for nested schema](#nestedatt--config--forge))
- `head_job_cpus` (Number) Number of CPUs allocated for the head job
- `head_job_memory_mb` (Number) Memory allocation for the head job in MB
- `head_job_role` (String) IAM role ARN for the head job
- `head_queue` (String) Name of the head job queue
- `post_run_script` (String) Bash script to run after workflow execution completes.
Use for cleanup, archiving results, etc.
- `pre_run_script` (String) Bash script to run before workflow execution begins.
Use for environment setup, loading modules, etc.

<a id="nestedatt--config--forge"></a>
### Nested Schema for `config.forge`

Read-Only:

- `allocation_strategy` (String) Strategy for allocating compute resources.
SPOT_CAPACITY_OPTIMIZED only valid when forge_type is SPOT.
- `bid_percentage` (Number) Maximum percentage of On-Demand price to pay for Spot instances (0-100).
Only applicable when forge_type is SPOT.
- `dispose_on_deletion` (Boolean) Dispose of AWS Batch resources when compute environment is deleted
- `ebs_auto_scale` (Boolean) Enable automatic EBS volume expansion
- `ebs_block_size` (Number) Size of EBS root volume in GB (minimum 8 GB, maximum 16 TB)
- `ec2_key_pair` (String) EC2 key pair name for SSH access to compute instances
- `efs_create` (Boolean) Automatically create an EFS file system
- `efs_id` (String) EFS file system ID to mount.
Format: fs- followed by hexadecimal characters
- `efs_mount` (String) Path where EFS will be mounted in the container
- `fargate_head_enabled` (Boolean) Use Fargate for head job instead of EC2.
Reduces costs by running head job on serverless compute.
- `forge_type` (String) Type of compute instances to provision:
- SPOT: Use EC2 Spot instances (cost-effective, can be interrupted)
- EC2: Use On-Demand EC2 instances (reliable, higher cost)
- FARGATE: Use AWS Fargate serverless compute
- `fsx_mount` (String) Path where FSx will be mounted in the container
- `fsx_name` (String) FSx for Lustre file system name
- `fsx_size` (Number) Size of FSx file system in GB
- `gpu_enabled` (Boolean) Enable GPU support for compute instances.
When enabled, GPU-capable instance types will be selected.
- `instance_types` (List of String) List of EC2 instance types to use.
Examples: ["m5.xlarge", "m5.2xlarge"], ["c5.2xlarge"], ["p3.2xlarge"]
Default: ["optimal"] - AWS Batch selects appropriate instances
- `max_cpus` (Number) Maximum number of CPUs available in the compute environment.
Subject to AWS service quotas.
- `min_cpus` (Number) Minimum number of CPUs to maintain in the compute environment.
Setting to 0 allows environment to scale to zero when idle.
- `security_groups` (List of String) List of security group IDs to attach to compute instances
- `subnets` (List of String) List of subnet IDs for compute instances.
Subnets must be in the specified VPC. Use multiple subnets for high availability.
- `vpc_id` (String) VPC ID where compute environment will be deployed.
Format: vpc- followed by hexadecimal characters
